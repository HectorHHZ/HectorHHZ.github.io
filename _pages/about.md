---
permalink: /
title: "BIO"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a third-year PhD student in the School of Computer Science at Carnegie Mellon University(CMU), specializing in machine learning and computer software engineering. My research focuses on Large Language Model post-training, make the LLMs better (more align with domain specific tasks), faster (more efficient in training and inference), and cheaper (training with less GPU hours and GPU memory utilization).

At CMU, I am advised by Prof. [Heather Miller](https://heather.miller.am/). Previously, I earned my master in Computer Science from New York University advised by Prof. [Anna Choromanska](https://engineering.nyu.edu/faculty/anna-choromanska) and Prof. [Parijat Dube](https://scholar.google.com/citations?user=bOejjQUAAAAJ&hl=en). I received my B.S. in Computer Science and Engineering from The Chinese University of Hong Kong(CUHK) where I worked with Prof. [David Zhang, Dapeng](https://scholar.google.com/citations?hl=zh-CN&user=IOagLnEAAAAJ) and [Prof. Rui Huang](https://scholar.google.com/citations?user=t8UduWwAAAAJ&hl=zh-CN&oi=ao). Before starting my PhD, My research mainly focuses on distributed machine learning system. 

- (This personal website is updated as of March 2025.)





News
======
- 5-2025:  I started my internship at AWS-AI-Labs@Amazon working on LLM post-training enabled speculative decoding via latent space reasoning.
- 2-2025: Open-source [SMT](https://github.com/HectorHHZ/Sparse_Matrix_Tuning?tab=readme-ov-file). We implemented SMT in two frameworks: DeepSpeed and Hugging Face Trainer/PEFT.
- 2-2025: [SMT: Fine-Tuning Large Language Models with Sparse Matrices](https://openreview.net/forum?id=GbgCRJedQ7), has been accepted by ICLR 2025.ðŸŽ‰ðŸŽ‰
- 5-2024: [Adjacent Leader Decentralized Stochastic Gradient Descent](https://ebooks.iospress.nl/volumearticle/69872) has been accepted by ECAI 2024.
- I started my Ph.D. journey at CMU.
- 2- 2023: Open-source a  general [framework](https://github.com/HectorHHZ/Adjacent_Leader_Dencentralized_SGD) to implement any (de)centralized, (a)synchronous distributed SGD algorithms when models fit into a single machine. The paper, which proposes a novel distributed SGD algorithm. 



Selected Publications
======
<!-- - **Haoze He**, Xingyuan Ding, Xuan Jiang, Alex Cheng, Yibo Zhao, Juncheng Billy Li, Heather Miller, "*Fine-Tuning MoE Large Language Models with Condenser Experts*", *International Conference on Learning Representations (ICLR)*, Submitted, Sept. 2025. [[paper]] -->

- **Haoze He**, Juncheng Billy Li, Xuan Jiang, Heather Miller, "*[Sparse Matrix in Large Language Model Fine-Tuning](https://openreview.net/forum?id=GbgCRJedQ7)*", *International Conference on Learning Representations (ICLR)*, Accepted, Jan. 2025. [[code](https://github.com/HectorHHZ/Sparse_Matrix_Tuning/)]

- **Haoze He**, Jing Wang, Anna Choromanska, "*[Adjacent Leader Decentralized Stochastic Gradient Descent](https://ebooks.iospress.nl/volumearticle/69872)*", *European Conference on Artificial Intelligence (ECAI)*, Accepted, June 2024. [[code](https://github.com/HectorHHZ/Adjacent_Leader_Dencentralized_SGD)]


*My full publication list can be found on my [Google Scholar profile](https://scholar.google.com/citations?user=PKGTBOcAAAAJ&hl=en&oi=ao).*


Academic Blog
======
- Peter Zhong, **Haoze He**, Omar Khattab, Christopher Potts, Matei Zaharia, Heather Miller, "*[A Guide to Large Language Model Abstractions](https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/)*", Jan. 2024.


Education
======
- Ph.D. in Machine Learning and Software Engineering at Carnegie Mellon University, 2023-present
  - GPA: 4.16/4.0, Rank: top1%
- M.S. in Computer Engineering at New York University, 2021-2023
  - GPA: 3.93/4.0, Rank: top1%
- B.S. in Computer Science and Engineering at The Chinese University of Hong Kong, 2016-2020




Internship
======
- **Applied Research Scientist Intern**, *Amazon AWS AI Labs*, Summer 2025 



Awards
======
- **Presidential Fellowship**, *Carnegie Mellon University*, Nov. 2024  



Teaching
======
- **Teaching Assistant**, *Large Language Model Systems (CMU 11-868)*, Spring 2025  


Service
======
- **Reviewer**, *International Conference on Learning Representations (ICLR)* â€” 2025, 2026  
- **Reviewer**, *International Joint Conference on Neural Networks (IJCNN)* â€” 2025  
- **Reviewer**, *The Association for the Advancement of Artificial Intelligence (AAAI)* â€” 2025
- **Reviewer**, *International Conference on Acoustics, Speech, and Signal Processing (ICASSP)* â€” 2022 â€“ 2025  
- **Reviewer**, *International Conference on Computer Vision (ICCV) Workshops* â€” 2023




Open-Sources for the Community
======
- Build an **open-source [website](https://github.com/HectorHHZ/NYU-Course-Schedule)** for NYU EECS/DS community and help **150+** NYU students **each semester**. This website summary the open-source courses in NYU EECS/DS, provide links and repositories for each course, list the workload, and provide course experiences for reference. Anyone from the NYU community is welcome to fork and contribute!
